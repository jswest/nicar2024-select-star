{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bf974aa-3730-4059-8e02-3b7fc9b737a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictWriter\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c763cb5-85f8-4628-b513-f8b47f64b3ce",
   "metadata": {},
   "source": [
    "### Get each page of image metadata in order to scrape it.\n",
    "\n",
    "We want to start by scraping the universe of images that we _could_ get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa7a2756-5ef3-40a2-8485-4cc18a166d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [04:12<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# We'll start with some config.\n",
    "# The LoC website indicates that there are 170,989 images in this collection.\n",
    "# But we just need a toy dataset, so we'll going to grab 10,000 images.\n",
    "LOC_IMAGES_COUNT = 10000\n",
    "per_page_count = 100\n",
    "loc_pages_count = math.ceil(LOC_IMAGES_COUNT / 100)\n",
    "url_base = \"https://www.loc.gov/collections/fsa-owi-black-and-white-negatives\"\n",
    "\n",
    "# Loop to get each page.\n",
    "current_page = 0\n",
    "for index in tqdm(range(loc_pages_count)):\n",
    "    # Let's get the current page 1-indexed rather than zero-indexed.\n",
    "    current_page = index + 1\n",
    "\n",
    "    # Let's be really nice to the Library of Congress's API; it's a public good, after all.\n",
    "    sleep(1)\n",
    "\n",
    "    # Prepare the URL query parameters.\n",
    "    url_params = [\n",
    "        \"fo=json\", # We want JSON results.\n",
    "        f\"c={per_page_count}\", # We want 100 results at a time.\n",
    "        \"at=results\", # We want to limit to results only.\n",
    "        \"sb=date\", # Sort by date ascending.\n",
    "        f\"sp={current_page}\" # We want the 1-indexed search page.\n",
    "    ]\n",
    "\n",
    "    # Make the request.\n",
    "    response = requests.get(f\"{url_base}?{'&'.join(url_params)}\")\n",
    "\n",
    "    # Check that we're good, then save.\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Prepare to save the index JSON to disk.\n",
    "        current_page_string = f\"{current_page}\".zfill(6)\n",
    "        out_file_path = f\"./../data/index-page-{current_page_string}.json\"\n",
    "\n",
    "        with open(out_file_path, 'w') as out_file:\n",
    "            json.dump(data, out_file)\n",
    "    \n",
    "    # Uh oh, something went wrong.\n",
    "    else:\n",
    "        print(f\"Request failed with non-200 status code at page {current_page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e361a9-8870-42fd-aad1-1292b95192f9",
   "metadata": {},
   "source": [
    "### Combine each page of index data into a single JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d39c130c-fb7f-4c65-87c2-f98299d08650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 207.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to hold the data which we will save as a unified JSON file.\n",
    "out_data = []\n",
    "\n",
    "# Define the pattern we'll use to glob up all the JSON files.\n",
    "pattern = \"./../data/index-page-*.json\"\n",
    "\n",
    "for file_path in tqdm(glob.glob(pattern)):\n",
    "    with open(file_path, 'r') as in_file:\n",
    "        data = json.load(in_file)\n",
    "        out_data += data['results']\n",
    "\n",
    "with open(\"./../data/index-full.json\", \"w\") as out_file:\n",
    "    json.dump(out_data, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470eca0d-90f8-4c8c-b04b-ae1c9d292230",
   "metadata": {},
   "source": [
    "### Save selected fields as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4895a129-be27-4234-939b-a9886fae41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 477097.13it/s]\n"
     ]
    }
   ],
   "source": [
    "out_data = []\n",
    "\n",
    "with open(\"./../data/index-full.json\", \"r\") as in_file:\n",
    "    data = json.load(in_file)\n",
    "\n",
    "for item in tqdm(data):\n",
    "\n",
    "    conditions = [\n",
    "        # We want there to be an id field.\n",
    "        (\"id\" in item),\n",
    "\n",
    "        # We want there to be an image_url field.\n",
    "        (\"image_url\" in item),\n",
    "\n",
    "        # We want image_url to be a list.\n",
    "        (isinstance(item[\"image_url\"], list)),\n",
    "\n",
    "        # We want image_url to have more than zero entries with \"640\" pixels in it.\n",
    "        (len([url for url in item[\"image_url\"] if \"640\" in url]) > 0),\n",
    "\n",
    "        # We want additional metadata.\n",
    "        (\"item\" in item)\n",
    "    ]\n",
    "\n",
    "    should_continue = False\n",
    "    for condition in conditions:\n",
    "        if not condition:\n",
    "            should_continue = True\n",
    "\n",
    "    if should_continue:\n",
    "        continue\n",
    "\n",
    "    # Get who took the photo as best as we can.\n",
    "    creators = []\n",
    "    if \"creators\" in item[\"item\"]:\n",
    "        for creator in item[\"item\"][\"creators\"]:\n",
    "            creators.append(creator[\"title\"])\n",
    "    \n",
    "    out_data.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"image_url\": [url for url in item[\"image_url\"] if \"640\" in url][0],\n",
    "        \"date\": item[\"date\"],\n",
    "        \"title\": item[\"item\"][\"title\"],\n",
    "        \"creators\": \"; \".join(creators)\n",
    "    })\n",
    "\n",
    "# Write the result to CSV.\n",
    "with open(\"./../data/index-full.csv\", \"w\") as out_file:\n",
    "\n",
    "    # Get the keys--the headers--of the CSV file.\n",
    "    keys = list(out_data[0].keys())\n",
    "\n",
    "    # Make a DictWriter.\n",
    "    dict_writer = DictWriter(out_file, fieldnames=keys)\n",
    "\n",
    "    # Write the header.\n",
    "    dict_writer.writeheader()\n",
    "\n",
    "    # Let's go!\n",
    "    for row in out_data:\n",
    "        dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8d16b8c-e10d-40c4-b0d0-5cba3a7f9548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have a dataframe of 9829 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2000/2000 [19:57<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../data/index-full.csv\")\n",
    "print(f\"We now have a dataframe of {len(df)} images.\")\n",
    "\n",
    "out_data = []\n",
    "\n",
    "# Iterate over the first 1,000 images (we're just making a toy dataset, here!)\n",
    "for row in tqdm(df.to_dict('records')[0:2000]):\n",
    "    \n",
    "    # Get the image out path.\n",
    "    slug = row[\"id\"].split('/')[-2]\n",
    "    image_out_path = f\"./../data/image-{slug}.jpg\"\n",
    "\n",
    "    row[\"image_filename\"] = f\"{slug}.jpg\"\n",
    "    out_data.append(row)\n",
    "    \n",
    "    # If we've already scraped this path, skip.\n",
    "    if os.path.exists(image_out_path):\n",
    "        continue\n",
    "\n",
    "    # Actually make the request.\n",
    "    response = requests.get(row[\"image_url\"])\n",
    "\n",
    "    # Check that we're 200.\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Save the image.\n",
    "        with open(image_out_path, 'wb') as out_file:\n",
    "            out_file.write(response.content)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed on row with id: {id}.\")\n",
    "\n",
    "    # Let's be really nice to the Library of Congress's API; it's a public good, after all.\n",
    "    sleep(1)\n",
    "\n",
    "# Write the result to CSV.\n",
    "with open(\"./../data/index.csv\", \"w\") as out_file:\n",
    "\n",
    "    # Get the keys--the headers--of the CSV file.\n",
    "    keys = list(out_data[0].keys())\n",
    "\n",
    "    # Make a DictWriter.\n",
    "    dict_writer = DictWriter(out_file, fieldnames=keys)\n",
    "\n",
    "    # Write the header.\n",
    "    dict_writer.writeheader()\n",
    "\n",
    "    # Let's go!\n",
    "    for row in out_data:\n",
    "        dict_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56108c45-bd38-4519-ba85-b287fa496a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
